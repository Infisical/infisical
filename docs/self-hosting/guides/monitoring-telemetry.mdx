---
title: "Monitoring and Telemetry Setup"
description: "Learn how to set up monitoring and telemetry for your self-hosted Infisical instance using Grafana, Prometheus, and OpenTelemetry."
---

Infisical provides comprehensive monitoring and telemetry capabilities to help you monitor the health, performance, and usage of your self-hosted instance. This guide covers setting up monitoring using both traditional OpenTelemetry approaches and modern DataDog integration.

## Overview

Infisical exports metrics in **OpenTelemetry (OTEL) format**, which provides maximum flexibility for your monitoring infrastructure. While this guide focuses on Grafana, the OTEL format means you can easily integrate with:

- **Cloud-native monitoring**: AWS CloudWatch, Google Cloud Monitoring, Azure Monitor
- **Observability platforms**: Datadog, New Relic, Splunk, Dynatrace
- **Custom backends**: Any system that supports OTEL ingestion
- **Traditional monitoring**: Prometheus, Grafana (as covered in this guide)

Infisical supports three telemetry collection methods:

1. **Pull-based (Prometheus)**: Exposes metrics on a dedicated endpoint for Prometheus to scrape
2. **Push-based (OTLP)**: Sends metrics to an OpenTelemetry Collector via OTLP protocol
3. **DataDog Direct Integration**: Sends custom metrics directly to DataDog using DogStatsD protocol through a DataDog agent

All approaches provide comprehensive metrics data, so you can choose the one that best fits your infrastructure and monitoring strategy.

## Prerequisites

- Self-hosted Infisical instance running
- Access to deploy monitoring services (Prometheus, Grafana, etc.)
- Basic understanding of Prometheus and Grafana

## Environment Variables

Configure the following environment variables in your Infisical backend:

```bash
# Enable telemetry collection
OTEL_TELEMETRY_COLLECTION_ENABLED=true

# Choose export type: "prometheus" or "otlp"
OTEL_EXPORT_TYPE=prometheus

# For OTLP push mode, also configure:
# OTEL_EXPORT_OTLP_ENDPOINT=http://otel-collector:4318/v1/metrics
# OTEL_COLLECTOR_BASIC_AUTH_USERNAME=your_collector_username
# OTEL_COLLECTOR_BASIC_AUTH_PASSWORD=your_collector_password
# OTEL_OTLP_PUSH_INTERVAL=30000
```

**Note**: The `OTEL_COLLECTOR_BASIC_AUTH_USERNAME` and `OTEL_COLLECTOR_BASIC_AUTH_PASSWORD` values must match the credentials configured in your OpenTelemetry Collector's `basicauth/server` extension. These are not hardcoded values - you configure them in your collector configuration file.

## Option 1: Pull-based Monitoring (Prometheus)

This approach exposes metrics on port 9464 at the `/metrics` endpoint, allowing Prometheus to scrape the data. The metrics are exposed in Prometheus format but originate from OpenTelemetry instrumentation.

### Configuration

1. **Enable Prometheus export in Infisical**:

   ```bash
   OTEL_TELEMETRY_COLLECTION_ENABLED=true
   OTEL_EXPORT_TYPE=prometheus
   ```

2. **Expose the metrics port** in your Infisical backend:

   - **Docker**: Expose port 9464
   - **Kubernetes**: Create a service exposing port 9464
   - **Other**: Ensure port 9464 is accessible to your monitoring stack

3. **Create Prometheus configuration** (`prometheus.yml`):

   ```yaml
   global:
     scrape_interval: 30s
     evaluation_interval: 30s

   scrape_configs:
     - job_name: "infisical"
       scrape_interval: 30s
       static_configs:
         - targets: ["infisical-backend:9464"] # Adjust hostname/port based on your deployment
       metrics_path: "/metrics"
   ```

   **Note**: Replace `infisical-backend:9464` with the actual hostname and port where your Infisical backend is running. This could be:

   - **Docker Compose**: `infisical-backend:9464` (service name)
   - **Kubernetes**: `infisical-backend.default.svc.cluster.local:9464` (service name)
   - **Bare Metal**: `192.168.1.100:9464` (actual IP address)
   - **Cloud**: `your-infisical.example.com:9464` (domain name)

### Deployment Options

#### Docker Compose

```yaml
services:
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
```

#### Kubernetes

```yaml
# prometheus-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      containers:
        - name: prometheus
          image: prom/prometheus:latest
          ports:
            - containerPort: 9090
          volumeMounts:
            - name: config
              mountPath: /etc/prometheus
      volumes:
        - name: config
          configMap:
            name: prometheus-config

---
# prometheus-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: prometheus
spec:
  selector:
    app: prometheus
  ports:
    - port: 9090
      targetPort: 9090
  type: ClusterIP
```

#### Helm

```bash
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm install prometheus prometheus-community/prometheus \
  --set server.config.global.scrape_interval=30s \
  --set server.config.scrape_configs[0].job_name=infisical \
  --set server.config.scrape_configs[0].static_configs[0].targets[0]=infisical-backend:9464
```

## Option 2: Push-based Monitoring (OTLP)

This approach sends metrics directly to an OpenTelemetry Collector via the OTLP protocol. This gives you the most flexibility as you can configure the collector to export to multiple backends simultaneously.

### Configuration

1. **Enable OTLP export in Infisical**:

   ```bash
   OTEL_TELEMETRY_COLLECTION_ENABLED=true
   OTEL_EXPORT_TYPE=otlp
   OTEL_EXPORT_OTLP_ENDPOINT=http://otel-collector:4318/v1/metrics
   OTEL_COLLECTOR_BASIC_AUTH_USERNAME=infisical
   OTEL_COLLECTOR_BASIC_AUTH_PASSWORD=infisical
   OTEL_OTLP_PUSH_INTERVAL=30000
   ```

2. **Create OpenTelemetry Collector configuration** (`otel-collector-config.yaml`):

   ```yaml
   extensions:
     health_check:
     pprof:
     zpages:
     basicauth/server:
       htpasswd:
         inline: |
           your_username:your_password

   receivers:
     otlp:
       protocols:
         http:
           endpoint: 0.0.0.0:4318
           auth:
             authenticator: basicauth/server

     prometheus:
       config:
         scrape_configs:
           - job_name: otel-collector
             scrape_interval: 30s
             static_configs:
               - targets: [infisical-backend:9464]
             metric_relabel_configs:
               - action: labeldrop
                 regex: "service_instance_id|service_name"

   processors:
     batch:

   exporters:
     prometheus:
       endpoint: "0.0.0.0:8889"
       auth:
         authenticator: basicauth/server
       resource_to_telemetry_conversion:
         enabled: true

   service:
     extensions: [basicauth/server, health_check, pprof, zpages]
     pipelines:
       metrics:
         receivers: [otlp]
         processors: [batch]
         exporters: [prometheus]
   ```

   **Important**: Replace `your_username:your_password` with your chosen credentials. These must match the values you set in Infisical's `OTEL_COLLECTOR_BASIC_AUTH_USERNAME` and `OTEL_COLLECTOR_BASIC_AUTH_PASSWORD` environment variables.

3. **Create Prometheus configuration** for the collector:

   ```yaml
   global:
     scrape_interval: 30s
     evaluation_interval: 30s

   scrape_configs:
     - job_name: "otel-collector"
       scrape_interval: 30s
       static_configs:
         - targets: ["otel-collector:8889"] # Adjust hostname/port based on your deployment
       metrics_path: "/metrics"
   ```

   **Note**: Replace `otel-collector:8889` with the actual hostname and port where your OpenTelemetry Collector is running. This could be:

   - **Docker Compose**: `otel-collector:8889` (service name)
   - **Kubernetes**: `otel-collector.default.svc.cluster.local:8889` (service name)
   - **Bare Metal**: `192.168.1.100:8889` (actual IP address)
   - **Cloud**: `your-collector.example.com:8889` (domain name)

### Deployment Options

#### Docker Compose

```yaml
services:
  otel-collector:
    image: otel/opentelemetry-collector-contrib:latest
    ports:
      - 4318:4318 # OTLP http receiver
      - 8889:8889 # Prometheus exporter metrics
    volumes:
      - ./otel-collector-config.yaml:/etc/otelcol-contrib/config.yaml:ro
    command:
      - "--config=/etc/otelcol-contrib/config.yaml"
```

#### Kubernetes

```yaml
# otel-collector-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otel-collector
spec:
  replicas: 1
  selector:
    matchLabels:
      app: otel-collector
  template:
    metadata:
      labels:
        app: otel-collector
    spec:
      containers:
        - name: otel-collector
          image: otel/opentelemetry-collector-contrib:latest
          ports:
            - containerPort: 4318
            - containerPort: 8889
          volumeMounts:
            - name: config
              mountPath: /etc/otelcol-contrib
      volumes:
        - name: config
          configMap:
            name: otel-collector-config
```

#### Helm

```bash
helm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts
helm install otel-collector open-telemetry/opentelemetry-collector \
  --set config.receivers.otlp.protocols.http.endpoint=0.0.0.0:4318 \
  --set config.exporters.prometheus.endpoint=0.0.0.0:8889
```

## Alternative Backends

Since Infisical exports in OpenTelemetry format, you can easily configure the collector to send metrics to other backends instead of (or in addition to) Prometheus:

### Cloud-Native Examples

```yaml
# Add to your otel-collector-config.yaml exporters section
exporters:
  # AWS CloudWatch
  awsemf:
    region: us-west-2
    log_group_name: /aws/emf/infisical
    log_stream_name: metrics

  # Google Cloud Monitoring
  googlecloud:
    project_id: your-project-id

  # Azure Monitor
  azuremonitor:
    connection_string: "your-connection-string"

  # Datadog
  datadog:
    api:
      key: "your-api-key"
      site: "datadoghq.com"

  # New Relic
  newrelic:
    apikey: "your-api-key"
    host_override: "otlp.nr-data.net"
```

### Multi-Backend Configuration

```yaml
service:
  pipelines:
    metrics:
      receivers: [otlp]
      processors: [batch]
      exporters: [prometheus, awsemf, datadog] # Send to multiple backends
```

## Option 3: DataDog Agent Integration

This approach sends custom metrics to DataDog using the DogStatsD protocol through a DataDog agent. This is the recommended approach for DataDog users as it provides better performance and reliability.

### Configuration

1. **Install and configure DataDog Agent**:

   Follow the [DataDog Agent installation guide](https://docs.datadoghq.com/agent/) for your platform. The agent must be running and accessible from your Infisical instance.

2. **Enable DataDog telemetry in Infisical**:

   ```bash
   DATADOG_ENABLED=true
   DATADOG_AGENT_HOST=localhost  # DataDog agent host
   DATADOG_AGENT_PORT=8125       # DogStatsD port (default: 8125)
   DATADOG_SERVICE_NAME=infisical
   DATADOG_SERVICE_VERSION=1.0.0
   DATADOG_TELEMETRY_ENV=production  # Optional, defaults to development
   ```

3. **Configure DataDog Agent**:

   Ensure your DataDog agent has DogStatsD enabled (this is the default). In your `datadog.yaml` configuration:

   ```yaml
   # Enable DogStatsD
   dogstatsd_enabled: true
   dogstatsd_port: 8125
   dogstatsd_non_local_traffic: false  # Set to true if Infisical runs on different host
   ```

### Environment Variables Reference

| Variable | Required | Default | Description |
|----------|----------|---------|-------------|
| `DATADOG_ENABLED` | Yes | `false` | Enable/disable DataDog telemetry |
| `DATADOG_AGENT_HOST` | No | `localhost` | DataDog agent hostname or IP address |
| `DATADOG_AGENT_PORT` | No | `8125` | DogStatsD port on the DataDog agent |
| `DATADOG_SERVICE_NAME` | No | `infisical` | Service name for metrics |
| `DATADOG_SERVICE_VERSION` | No | `1.0.0` | Service version tag |
| `DATADOG_TELEMETRY_ENV` | No | `development` | Environment tag |

### Deployment Examples

#### Docker Compose

```yaml
services:
  datadog-agent:
    image: datadog/agent:latest
    environment:
      - DD_API_KEY=${DD_API_KEY}
      - DD_SITE=${DD_SITE:-datadoghq.com}
      - DD_DOGSTATSD_NON_LOCAL_TRAFFIC=true
    ports:
      - "8125:8125/udp"  # DogStatsD
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /proc/:/host/proc/:ro
      - /sys/fs/cgroup/:/host/sys/fs/cgroup:ro

  infisical-backend:
    # ... other configuration
    environment:
      - DATADOG_ENABLED=true
      - DATADOG_AGENT_HOST=datadog-agent
      - DATADOG_AGENT_PORT=8125
      - DATADOG_SERVICE_NAME=infisical
      - DATADOG_SERVICE_VERSION=1.0.0
      - DATADOG_TELEMETRY_ENV=production
    depends_on:
      - datadog-agent
```

#### Kubernetes

First, install the DataDog agent using the official Helm chart:

```bash
helm repo add datadog https://helm.datadoghq.com
helm install datadog datadog/datadog \
  --set datadog.apiKey=${DD_API_KEY} \
  --set datadog.site=${DD_SITE:-datadoghq.com}
```

Then configure your Infisical deployment:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: infisical-backend
spec:
  template:
    spec:
      containers:
      - name: infisical
        env:
        - name: DATADOG_ENABLED
          value: "true"
        - name: DATADOG_AGENT_HOST
          value: "datadog.default.svc.cluster.local"  # DataDog agent service
        - name: DATADOG_AGENT_PORT
          value: "8125"
        - name: DATADOG_SERVICE_NAME
          value: "infisical"
        - name: DATADOG_SERVICE_VERSION
          value: "1.0.0"
        - name: DATADOG_TELEMETRY_ENV
          value: "production"
```

### Available Metrics in DataDog

When DataDog integration is enabled, Infisical will send the following custom metrics:

#### Secret Sync Error Metrics

- `infisical.secret_sync.operation_errors` - Errors during secret synchronization operations
- `infisical.secret_sync.import_errors` - Errors during secret import operations
- `infisical.secret_sync.removal_errors` - Errors during secret removal operations

Each metric includes the following tags:
- `service` - Service name (configured via `DATADOG_SERVICE_NAME`)
- `env` - Environment (configured via `DATADOG_TELEMETRY_ENV`)
- `version` - Service version (configured via `DATADOG_SERVICE_VERSION`)
- `destination` - Target system for the sync operation
- `sync_id` - Unique identifier for the sync configuration
- `project_id` - Infisical project identifier
- `error_type` - Type of error (e.g., "AxiosError", "ValidationError")
- `error_status` - HTTP status code (when applicable)
- `error_name` - Specific error name

### Setting Up DataDog Dashboards

1. **Access DataDog Dashboards**:
   - Log into your DataDog account
   - Navigate to **Dashboards** → **New Dashboard**

2. **Create Secret Sync Monitoring Dashboard**:
   ```json
   {
     "title": "Infisical Secret Sync Monitoring",
     "widgets": [
       {
         "definition": {
           "type": "timeseries",
           "requests": [
             {
               "q": "sum:infisical.secret_sync.operation_errors{*} by {destination,error_type}",
               "display_type": "bars"
             }
           ],
           "title": "Secret Sync Errors by Destination"
         }
       },
       {
         "definition": {
           "type": "query_value",
           "requests": [
             {
               "q": "sum:infisical.secret_sync.operation_errors{*}.as_count()",
               "aggregator": "sum"
             }
           ],
           "title": "Total Sync Errors (24h)"
         }
       }
     ]
   }
   ```

3. **Set Up Alerts**:
   - Navigate to **Monitors** → **New Monitor**
   - Choose **Metric** monitor type
   - Configure alert condition: `sum:infisical.secret_sync.operation_errors{*}.as_count() > 5`
   - Set notification preferences

### Troubleshooting DataDog Integration

1. **Metrics not appearing in DataDog**:
   - Verify `DATADOG_ENABLED=true`
   - Check DataDog API key validity
   - Ensure correct DataDog site configuration
   - Check Infisical logs for DataDog initialization messages

2. **Authentication errors**:
   - Validate your DataDog API key
   - Ensure API key has the necessary permissions for metric submission

3. **Regional endpoint issues**:
   - Confirm your DataDog account region matches the configured site
   - Verify the OTLP endpoint URL for your region

4. **Missing tags or incorrect data**:
   - Check environment variable configuration
   - Verify service name and version settings```

## Setting Up Grafana

1. **Access Grafana**: Navigate to your Grafana instance
2. **Login**: Use your configured credentials
3. **Add Prometheus Data Source**:
   - Go to Configuration → Data Sources
   - Click "Add data source"
   - Select "Prometheus"
   - Set URL to your Prometheus endpoint
   - Click "Save & Test"

## Available Metrics

Infisical exposes the following key metrics in OpenTelemetry format:

### API Performance Metrics

- `API_latency` - API request latency histogram in milliseconds

  - **Labels**: `route`, `method`, `statusCode`
  - **Example**: Monitor response times for specific endpoints

- `API_errors` - API error count histogram
  - **Labels**: `route`, `method`, `type`, `name`
  - **Example**: Track error rates by endpoint and error type

### Integration & Secret Sync Metrics

- `integration_secret_sync_errors` - Integration secret sync error count

  - **Labels**: `version`, `integration`, `integrationId`, `type`, `status`, `name`, `projectId`
  - **Example**: Monitor integration sync failures across different services

- `secret_sync_sync_secrets_errors` - Secret sync operation error count

  - **Labels**: `version`, `destination`, `syncId`, `projectId`, `type`, `status`, `name`
  - **Example**: Track secret sync failures to external systems

- `secret_sync_import_secrets_errors` - Secret import operation error count

  - **Labels**: `version`, `destination`, `syncId`, `projectId`, `type`, `status`, `name`
  - **Example**: Monitor secret import failures

- `secret_sync_remove_secrets_errors` - Secret removal operation error count
  - **Labels**: `version`, `destination`, `syncId`, `projectId`, `type`, `status`, `name`
  - **Example**: Track secret removal operation failures

### System Metrics

These metrics are automatically collected by OpenTelemetry's HTTP instrumentation:

- `http_server_duration` - HTTP server request duration metrics (histogram buckets, count, sum)
- `http_client_duration` - HTTP client request duration metrics (histogram buckets, count, sum)

### Custom Business Metrics

- `infisical_secret_operations_total` - Total secret operations
- `infisical_secrets_processed_total` - Total secrets processed

## Troubleshooting

### Common Issues

1. **Metrics not appearing**:

   - Check if `OTEL_TELEMETRY_COLLECTION_ENABLED=true`
   - Verify the correct `OTEL_EXPORT_TYPE` is set
   - Check network connectivity between services

2. **Authentication errors**:

   - Verify basic auth credentials in OTLP configuration
   - Check if credentials match between Infisical and collector
